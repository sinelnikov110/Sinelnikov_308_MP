{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec59b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860c5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "nameOfTag = newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6910bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4700\n",
    "n_components = 20\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                    lowercase=True, stop_words=_stop_words.ENGLISH_STOP_WORDS,\n",
    "                    analyzer='word', binary=True,\n",
    "                    max_df=0.95, min_df=2,\n",
    "                    max_features=n_features\n",
    ")\n",
    "# одновременно создали словарь и преобразовали строку в вектор\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8ac2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4700"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f303db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _customLDA(n_d_k, n_k_w, n_k, _z, _document, _word, _alpha, _beta, _topic,  max_iter=10):\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        for j in range(len(_document)):\n",
    "            cur_word = _word[j]\n",
    "            cur_document = _document[j]\n",
    "            cur_topic = _z[j]\n",
    "            n_d_k[cur_document, cur_topic] -= 1\n",
    "            n_k_w[cur_topic, cur_word] -= 1\n",
    "            n_k[cur_topic] -= 1\n",
    "            p = (n_d_k[cur_document, :] + _alpha) * (n_k_w[:, cur_word] + _beta[cur_word]) / (n_k + _beta.sum())\n",
    "            _z[j] = np.random.choice(np.arange(_topic), p = p / p.sum())\n",
    "            n_d_k[cur_document, _z[j]] += 1\n",
    "            n_k_w[_z[j], cur_word] += 1\n",
    "            n_k[_z[j]] += 1\n",
    "    return n_d_k, n_k_w, n_k, _z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6e6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 20\n",
    "n_d_k = np.zeros( topic * X_train.shape[0]).reshape(X_train.shape[0], topic)\n",
    "n_k_w = np.zeros( topic * X_train.shape[1]).reshape(topic, X_train.shape[1])\n",
    "n_k = np.zeros(topic)\n",
    "document, word = X_train.nonzero()\n",
    "z = np.random.choice(topic, len(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66c6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, k in zip(document, word, z):\n",
    "    n_d_k[i, k] += 1\n",
    "    n_k_w[k, j] += 1\n",
    "    n_k[k] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1592046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [09:28<00:00, 18.95s/it]\n"
     ]
    }
   ],
   "source": [
    "n_d_k, n_k_w,  n_k, z = _customLDA(n_d_k, n_k_w, n_k, z, document, word, np.ones(20), np.ones(X_train.shape[1]), 20, max_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2572144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 1 \tanswer\tdifferent\tfar\tnew\tposting\treally\tsay\tsure\tthink\tway\n",
      "Tag 2 \t16\tapple\tbit\tboard\tcard\tmemory\tsoftware\tspeed\tsupport\tvideo\n",
      "Tag 3 \t10\t11\t15\t18\t30\tgame\tgames\tplay\tteam\tyear\n",
      "Tag 4 \tearth\thigh\tlarge\tlow\tnasa\tpower\tsmall\tspace\ttime\tuse\n",
      "Tag 5 \tcome\tdoes\tjust\tknow\tll\tlook\tpeople\tright\tthink\tway\n",
      "Tag 6 \tchildren\tcountry\tgovernment\tisrael\tjews\tkilled\tpeople\tstate\twar\tworld\n",
      "Tag 7 \tchip\tclipper\tdon\tencryption\tgovernment\tjust\tkey\tlaw\tpeople\tuse\n",
      "Tag 8 \tavailable\tcode\tfile\tfiles\tfollowing\tftp\tlist\tprogram\tusing\twrite\n",
      "Tag 9 \tdoes\tdon\tgood\tknow\tlike\tlook\tlooking\ttime\tuse\twant\n",
      "Tag 10 \tbuy\tcar\tgood\tinterested\tnew\toffer\tprice\tsale\tsell\tused\n",
      "Tag 11 \tbelieve\tbible\tchristian\tchristians\tgod\tjesus\tlife\tpeople\treligion\tsay\n",
      "Tag 12 \tamerican\tapril\tclinton\tfederal\tgovernment\tnational\tpresident\tpublic\tstate\tstates\n",
      "Tag 13 \tago\tday\tdon\tedu\tgood\tknow\tlike\tneed\tsoon\tyears\n",
      "Tag 14 \tagree\tcase\tdid\tdoes\tdoesn\tpoint\tsay\tthink\ttry\twrong\n",
      "Tag 15 \tbetter\tdon\tjust\tlike\tlot\tprobably\treally\tthink\tve\tway\n",
      "Tag 16 \tdid\tdon\tgot\tjust\tknow\tlater\tlet\tproblem\tthings\ttime\n",
      "Tag 17 \tadvance\tdoes\tdrive\thelp\thi\tproblem\tthanks\tuse\tusing\twindows\n",
      "Tag 18 \tdon\tgood\tjust\tknow\tlike\tmake\treally\ttry\tuse\twant\n",
      "Tag 19 \taddress\tca\tcom\tedu\temail\tinformation\tinternet\tmail\tsend\tuniversity\n",
      "Tag 20 \tdidn\tdon\tgoing\tjust\tknow\tlet\tlike\tright\tthing\tthink\n"
     ]
    }
   ],
   "source": [
    "result = np.argsort(n_k_w, axis=1)[:, -10:]\n",
    "for i in range(20):\n",
    "    matrix = np.zeros((1, X_train.shape[1]))\n",
    "    for j in result[i]:\n",
    "        matrix[0, j] = 1\n",
    "    print('Tag {} \\t{}'.format(i + 1, '\\t'.join(vectorizer.inverse_transform(matrix)[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00cb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
